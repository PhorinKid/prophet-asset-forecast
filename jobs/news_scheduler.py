import requests
import pandas as pd
from sqlalchemy import create_engine, Column, Integer, String, Date, Text, Float, TIMESTAMP, ForeignKey, func, text
from sqlalchemy.orm import declarative_base, sessionmaker
from bs4 import BeautifulSoup
import openai
import time
import json
from datetime import datetime
import config
from apscheduler.schedulers.blocking import BlockingScheduler

# ==============================================================================
# [1] ì„¤ì •
# ==============================================================================

# 1. ë°ì´í„°ë² ì´ìŠ¤ ì ‘ì† ì •ë³´
DB_CONFIG = config.DB_CONFIG
LOSTARK_API_TOKEN = config.LOSTARK_API_TOKEN
OPENAI_API_KEY = config.OPENAI_API_KEY
TARGET_CATEGORIES = config.TARGET_CATEGORIES

# ==============================================================================
# [2] ì´ˆê¸°í™”
# ==============================================================================

db_url = f'mysql+pymysql://{DB_CONFIG["user"]}:{DB_CONFIG["password"]}@{DB_CONFIG["host"]}:{DB_CONFIG["port"]}/{DB_CONFIG["db"]}'
engine = create_engine(db_url)
Session = sessionmaker(bind=engine)
Base = declarative_base()

class RawNotice(Base):
    __tablename__ = 'raw_notices'
    id = Column(Integer, primary_key=True, autoincrement=True)
    notice_date = Column(Date, nullable=False)
    title = Column(String(255), nullable=False)
    link = Column(String(500), nullable=False)
    content = Column(Text, nullable=True)
    created_at = Column(TIMESTAMP, server_default=func.now())

class ItemNoticeImpact(Base):
    __tablename__ = 'item_notice_impacts'
    id = Column(Integer, primary_key=True, autoincrement=True)
    notice_id = Column(Integer, ForeignKey('raw_notices.id'), nullable=False)
    item_id = Column(Integer, nullable=False)
    gpt_score = Column(Float, default=0.0)
    demand_pressure = Column(Float, default=0.0)
    supply_pressure = Column(Float, default=0.0)
    behavior_pressure = Column(Float, default=0.0)
    impact_days = Column(Integer, default=0)
    analyzed_at = Column(TIMESTAMP, server_default=func.now())

client = openai.OpenAI(api_key=OPENAI_API_KEY)

# ==============================================================================
# [3] ë¡œì§
# ==============================================================================

def scrape_notice_content(url):
    """ ë³¸ë¬¸ ì „ì²´ ìˆ˜ì§‘ (60,000ì) """
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            content_div = soup.find('div', class_='fr-view')
            if not content_div:
                content_div = soup.find('div', class_='article-content')
            
            if content_div:
                text = content_div.get_text(separator='\n')
                clean_text = '\n'.join([line.strip() for line in text.splitlines() if line.strip()])
                return clean_text[:60000]
        return ""
    except Exception as e:
        print(f"í¬ë¡¤ë§ ì—ëŸ¬: {e}")
        return ""

def get_gpt_score(title, content, item_name):
    # [Expert ML Signal Prompt] ì›ë³¸ í”„ë¡¬í”„íŠ¸ ì ìš©
    prompt = f"""
    You are an economic signal generator for a time-series prediction model.

    Your task is NOT to summarize patch notes,
    but to convert game announcements into quantitative market signals
    that can be used as exogenous variables in ML/DL models.

    Target item: {item_name}

    Time horizon:
    - Short-term impact only (1 to 7 days)
    - Ignore long-term meta effects

    Based on the announcement below, estimate the following signals:

    1. Demand pressure on the item
    2. Supply pressure on the item
    3. Behavioral / sentiment pressure (panic buy, sell-off, hoarding)

    Scoring rules:
    - Each signal must be a float between -1.0 and 1.0
    - Positive = price upward pressure
    - Negative = price downward pressure
    - 0.0 = no meaningful impact

    If the announcement is mainly bug fixes or maintenance:
    - All signals must be 0.0

    Also estimate:
    4. Impact duration (number of days, integer 0â€“7)

    Announcement title:
    {title}

    Announcement content:
    {content[:60000]}

    Output JSON ONLY in the following format:
    {{
        "demand_pressure": <float>,
        "supply_pressure": <float>,
        "behavior_pressure": <float>,
        "impact_days": <int>,
        "total_score": <float>,
        "reason": "brief causal explanation (Must be written in KOREAN)"
    }}

    Rules:
    - total_score must be a weighted result of the three pressures
    - Do NOT speculate beyond the announcement text
    - If uncertain, reduce magnitude toward 0
    - IMPORTANT: The 'reason' field MUST be written in Korean.
    """
    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.0
        )
        clean_json = response.choices[0].message.content.strip().replace("```json", "").replace("```", "")
        return json.loads(clean_json)
    except Exception as e:
        print(f"GPT Error: {e}")
        return {
            "demand_pressure": 0.0, "supply_pressure": 0.0, "behavior_pressure": 0.0,
            "impact_days": 0, "total_score": 0.0, "reason": "Error"
        }

def run_scheduler():
    print(f"\n[Scheduler Job Started] {datetime.now()}")
    session = Session()

    try:
        api_url = 'https://developer-lostark.game.onstove.com/news/notices?type=ê³µì§€&searchText=ì—…ë°ì´íŠ¸'
        headers = {'authorization': f'bearer {LOSTARK_API_TOKEN}'}
        res = requests.get(api_url, headers=headers)
        
        if res.status_code != 200:
            print("API í˜¸ì¶œ ì‹¤íŒ¨")
            return

        notices = res.json()
        target_notices = notices[:3]

        for notice_data in target_notices:
            n_date = pd.to_datetime(notice_data['Date']).date()
            n_title = notice_data['Title']
            n_link = notice_data['Link']

            existing = session.query(RawNotice).filter_by(title=n_title, notice_date=n_date).first()
            notice_id = None
            content_text = ""

            if not existing:
                print(f"ì‹ ê·œ ê³µì§€: {n_title}")
                content_text = scrape_notice_content(n_link)
                new_notice = RawNotice(notice_date=n_date, title=n_title, link=n_link, content=content_text)
                session.add(new_notice)
                session.commit()
                notice_id = new_notice.id
            else:
                print(f"ê¸°ì¡´ ê³µì§€: {n_title}")
                notice_id = existing.id
                content_text = existing.content

            # ì•„ì´í…œ ë¶„ì„
            items_sql = text("SELECT id, name FROM market_items WHERE category_code IN :cats")
            target_items = session.execute(items_sql, {"cats": tuple(TARGET_CATEGORIES)}).fetchall()

            print(f"-- ì•„ì´í…œ {len(target_items)}ê°œ ë¶„ì„ ì²´í¬...")
            
            analyze_count = 0
            for row in target_items:
                item_id = row[0]
                item_name = row[1]

                score_exists = session.query(ItemNoticeImpact).filter_by(notice_id=notice_id, item_id=item_id).first()
                
                if not score_exists:
                    result = get_gpt_score(n_title, content_text, item_name)
                    
                    impact = ItemNoticeImpact(
                        notice_id=notice_id,
                        item_id=item_id,
                        gpt_score=result['total_score'],
                        demand_pressure=result['demand_pressure'],
                        supply_pressure=result['supply_pressure'],
                        behavior_pressure=result['behavior_pressure'],
                        impact_days=result['impact_days']
                    )
                    session.add(impact)
                    session.commit()
                    analyze_count += 1
                    
                    if result['total_score'] != 0.0:
                        print(f"      âœ¨ {item_name}: {result['total_score']} ({result['reason']})")
                    
                    time.sleep(0.05)

            if analyze_count > 0:
                print(f"-- {analyze_count}ê°œ ì‹ ê·œ ë¶„ì„ ì™„ë£Œ")

    except Exception as e:
        print(f"ì—ëŸ¬: {e}")
    finally:
        session.close()
        print("ìŠ¤ì¼€ì¤„ëŸ¬ ì¢…ë£Œ")

if __name__ == "__main__":
    # íƒ€ì„ì¡´ ì„¤ì • (ì•„ì£¼ ì˜í•˜ì…¨ìŠµë‹ˆë‹¤!)
    scheduler = BlockingScheduler(timezone='Asia/Seoul')
    
    # ìŠ¤ì¼€ì¤„ ë“±ë¡: ë§¤ì£¼ ìˆ˜ìš”ì¼ 10ì‹œ 01ë¶„
    scheduler.add_job(run_scheduler, 'cron', day_of_week='wed', hour=10, minute=1)
    
    print("â° íŒŒì´ì¬ ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì • ì™„ë£Œ (ë§¤ì£¼ ìˆ˜ìš”ì¼ 10:01)")

    # ---------------------------------------------------------
    # [ì¶”ê°€] ë°°í¬ ì§í›„ ì˜ ëŒì•„ê°€ëŠ”ì§€ 'ì§€ê¸ˆ ë‹¹ì¥' í•œ ë²ˆ ì‹¤í–‰í•´ë³´ê¸°
    # ---------------------------------------------------------
    print("ğŸš€ [Self-Test] ì„œë²„ ì‹œì‘ ì§í›„ ìµœì´ˆ 1íšŒ ì‹¤í–‰ ì¤‘...")
    try:
        run_scheduler() # ì—¬ê¸°ì„œ ì—ëŸ¬ ë‚˜ë©´ ë°”ë¡œ ì•Œ ìˆ˜ ìˆìŒ!
    except Exception as e:
        print(f"âŒ ì´ˆê¸° ì‹¤í–‰ ì‹¤íŒ¨: {e}")

    # ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ (Blocking)
    print("âœ… ì´ˆê¸° ì‹¤í–‰ ì™„ë£Œ. ìŠ¤ì¼€ì¤„ëŸ¬ ëŒ€ê¸° ëª¨ë“œ ì§„ì…...")
    try:
        scheduler.start()
    except (KeyboardInterrupt, SystemExit):
        pass
    